{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 - Bias in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to explore the concept of 'bias' through data on Wikipedia articles on political figures from different countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes the set of political articles on Wikipedia, the predicted article quality scores for those articles, and a dataset of country populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series of plots will be as follows:  \n",
    "1) the countries with the greatest and least coverage of politicians on Wikipedia compared to their population.  \n",
    "2) the countries with the highest and lowest proportion of high quality articles about politicians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wikipedia dataset can be found on Figshare. This was extracted using the Wikimedia API, saved as a csv file named page_data.csv. The columns in this file are:    \n",
    "1) country: the name of the country  \n",
    "2) page: the wikipedia article title  \n",
    "3) rev_id: the revision id for the last edit of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This cell reads the page_data csv file into the dataframe \"data\" from the file page_data.csv.\n",
    "    The shape of this dataframe is (47197,3).\n",
    "    \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('page_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Population dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population data is from Population Research Bureau website.http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=14. The dataset has 5 columns: Location, Location Type, TimeFrame, Data Type, Data and Footnotes. I removed the unnecessary columns so that I could focus on the analysis part of the project without any superfluous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop = pd.read_csv('Population_Mid_2015.csv',header=1)\n",
    "del pop['Location Type']\n",
    "del pop['TimeFrame']\n",
    "del pop['Data Type']\n",
    "del pop['Footnotes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the merging of the two dataframes easier, I changed the 'Location' column name to 'country'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop.columns = ['country','population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop['population'] = pop['population'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop['population'] = [int(x) for x in pop['population'].values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article quality prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted quality scores for each article in the Wikipedia dataset comes from a Wikimedia API endpoint for a machine learning system called ORES (\"Objective Revision Evaluation Service\"). ORES estimates the quality of an article at a particular point in time, and assigns a series of probabilities that the article is best described by one of the categories listed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of quality scores are, from best to worst:  \n",
    "FA - Featured article  \n",
    "GA - Good article  \n",
    "B - B-class article  \n",
    "C - C-class article  \n",
    "Start - Start-class article  \n",
    "Stub - Stub-class article  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'User-Agent' : 'https://github.com/your_github_username', 'From' : 'your_uw_email@uw.edu'}\n",
    "\"\"\"\n",
    "    Taken from the example jupyter notebook, this function returns a json object containing the ORES data \n",
    "    for each article with revision id as 'revids'. \n",
    "    An example of the json object is as follows:\n",
    "    {'enwiki': {'models': {'wp10': {'version': '0.5.0'}},\n",
    "  'scores': {'235107991': {'wp10': {'score': {'prediction': 'Stub',\n",
    "      'probability': {'B': 0.00617021706415532,\n",
    "       'C': 0.01705290459462909,\n",
    "       'FA': 0.0015941304170005732,\n",
    "       'GA': 0.0012422843354764665,\n",
    "       'Start': 0.024596904658825667,\n",
    "       'Stub': 0.9493435589299127}}}}}}}\n",
    "\"\"\"\n",
    "\n",
    "def get_ores_data(revision_ids, headers):\n",
    "    \n",
    "    # Define the endpoint\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "    \n",
    "    params = {'project' : 'enwiki',\n",
    "              'model'   : 'wp10',\n",
    "              'revids'  : '|'.join(str(x) for x in revision_ids)\n",
    "              }\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    response = api_call.json()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ores_data(revision_ids,headers):\n",
    "    missing_ids = []\n",
    "    scores = [] #aq = article quality\n",
    "    endpoint = \"https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}\"    \n",
    "    params = {\"project\" : \"enwiki\",\n",
    "              \"model\" : \"wp10\",\n",
    "              \"revids\" : \"|\".join(str(x) for x in revision_ids)\n",
    "             }\n",
    "\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    \n",
    "    response = api_call.json()\n",
    "    for rev_id in revision_ids:\n",
    "        try:\n",
    "            scores.append(response[\"enwiki\"][\"scores\"][str(rev_id)][\"wp10\"][\"score\"][\"prediction\"])\n",
    "        except:\n",
    "            missing_ids.append(rev_id)\n",
    "            print(\"no rev_id found\")\n",
    "            pass\n",
    "    \n",
    "    return scores, missing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq=[]\n",
    "for i in range(0, len(data[\"rev_id\"]), 100):\n",
    "\n",
    "        revision_ids = data[\"rev_id\"][i:(i + 100)]    \n",
    "    \n",
    "        scores, missing_ids = get_ores_data(revision_ids,headers) \n",
    "        for score in scores:\n",
    "            aq.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the rows with the missing indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(46862, inplace=True)\n",
    "data.drop(46863, inplace=True)\n",
    "data.drop(45837, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47194, 3)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47230"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I have merged the data and the population dataframes for the final datafame which I will use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = data.merge(pop, on = 'country', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Renaming the columns of the dataframe to make it easier to perform analysis\n",
    "\"\"\"\n",
    "final_df.columns= ['article_name','country','rev_id','article_quality','population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2 - Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis calculates the proportion (as a percentage) of articles-per-population and high-quality articles for each country. \"High quality\" articles are defined as articles about politicians that ORES predicted would be in either the \"FA\" (featured article) or \"GA\" (good article) classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:  \n",
    ". if a country has a population of 10,000 people, and you found 10 articles about politicians from that country, then the percentage of articles-per-population would be .1%.  \n",
    ". if a country has 10 articles about politicians, and 2 of them are FA or GA class articles, then the percentage of high-quality articles would be 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting total number of articles for each country\n",
    "df_temp = final_df[['country','article_name']].groupby('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df_temp.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since population is an important factor in the analysis, we merge the population dataframe with the above data frame\n",
    "article_pop = df_temp.merge(pop,on='country',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_pop.columns = ['country','total_articles','population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculating the percentage of articles per country\n",
    "article_pop['perc_articles_pop']=article_pop['total_articles']/article_pop['population']\n",
    "article_pop['perc_articles_pop'] = article_pop['perc_articles_pop']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculating the proportion of high-quality (\"FA\" or \"GA\") articles for each country.\n",
    "hq = pd.concat([final_df.loc[final_df['article_quality']=='FA'], final_df.loc[final_df['article_quality']=='GA']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hq_count = hq.groupby('country').count()['article_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hq_count = hq_count.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hq_perc = pd.merge(hq_count,article_pop,on='country',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hq_perc['high_quality_perc'] = hq_perc['article_name']/hq_perc['total_articles']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hq_plot = hq_perc[['country','high_quality_perc']]\n",
    "hq_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First we sort the countries according to their high quality percentage values\n",
    "hq_asc = hq_plot.sort_values(['high_quality_perc'],ascending=True, inplace=False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 countries with the lowest percentage of high quality articles are\n",
    "hq_asc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hq_desc = hq_plot.sort_values(['high_quality_perc'],ascending=False, inplace=False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 countries with the highest percentage of high quality articles are\n",
    "hq_desc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
